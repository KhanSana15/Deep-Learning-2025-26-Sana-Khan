# -*- coding: utf-8 -*-
"""Machine Learning.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W4ZW5J1r1zmllUBzvQnbKAAIbaQAtiEy

# **SANA KHAN**
# **2410031378**
# **2CSE22**
"""

import pandas as pd
import numpy as np

headers = ["symboling","normalized-losses","make","fuel-type","aspiration", "num-of-doors","body-style",
         "drive-wheels","engine-location","wheel-base", "length","width","height","curb-weight","engine-type",
         "num-of-cylinders", "engine-size","fuel-system","bore","stroke","compression-ratio","horsepower",
         "peak-rpm","city-mpg","highway-mpg","price"]

ml= pd.read_csv('machinelearning.data', names= headers)

#replace ? to NaN

ml.replace("?", np.nan, inplace = True)

ml.head(5)

missing_data = ml.isnull()
missing_data.head(5)

for column in missing_data.columns.values.tolist():
    print (missing_data[column].value_counts())
    print("")

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')
imputer = imputer.fit(ml[['normalized-losses','stroke','bore','horsepower','peak-rpm']])
ml[['normalized-losses','stroke','bore','horsepower','peak-rpm']] = imputer.transform(ml[['normalized-losses','stroke','bore','horsepower','peak-rpm']])

imputer= SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')
imputer = imputer.fit(ml[['num-of-doors']])
ml[['num-of-doors']] = imputer.transform(ml[['num-of-doors']])

ml.head()

#simply drop whole row with NaN in "price" row
ml.dropna(subset=["price"], axis=0, inplace=True)

#reset index, cuz we dropped two rows
ml.reset_index(drop=True, inplace=True)

"""# **LINEAR REGRESSION**"""

import numpy as np
from sklearn.linear_model import LinearRegression

weight = np.array([50,60,70,80,90,100]).reshape(-1,1)
height = np.array([150,155,160,170,175,180])

model = LinearRegression()
model.fit(weight,height)

user_weight = float(input("Enter your weight in kg: "))
predicted_height = model.predict([[user_weight]])
print(f"Predicted height for {user_weight} kg: {predicted_height[0]} cm")

"""# **Best fit for LinearRegression**"""

import numpy as np
from sklearn.linear_model import LinearRegression
marks = np.array([0,10,22,33,40,49,76,80,93,100]).reshape(-1,1)
gradepoints = np.array([1,2,3,4,5,6,7,8,9,10])
model1 = LinearRegression()
model1.fit(marks,gradepoints)
user_marks = float(input("Enter your marks: "))
predicted_gradepoints = model1.predict([[user_marks]])
print(f"Predicted gradepoints for {user_marks} marks: {predicted_gradepoints[0]}")

"""# **LOGISTIC REGRESSION**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

data = {
    "Age": [22, 25, 27, 30, 35, 40, 45, 50, 55, 60],
    "Estimated_Salary": [25000, 30000, 35000, 40000, 50000, 60000, 70000, 80000, 85000, 90000],
    "Purchased": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
}
df = pd.DataFrame(data)
X = df[["Age", "Estimated_Salary"]]
y = df["Purchased"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
new_customer = pd.DataFrame([[28, 35000]], columns=["Age", "Estimated_Salary"])
prediction = model.predict(new_customer)
print("Prediction:", "Purchased" if prediction[0] == 1 else "Not Purchased")

"""# **Logistic regression in inbuilt dataset**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
df = pd.read_csv("car data.csv")
print("First 5 Rows:")
print(df.head())
print("\nAvailable Columns:", list(df.columns))
target_column = input("Enter the target column name (the one to predict): ")
X = df.drop(columns=[target_column])
y = df[target_column]
X = pd.get_dummies(X, drop_first=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
model = LogisticRegression(max_iter=1000)
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, zero_division=0))
ask = input("\nDo you want to predict for new input? (yes/no): ").lower()
if ask == "yes":
    print("\nEnter feature values in order:", list(X.columns))
    values = []
    for col in X.columns:
        val = float(input(f"{col}: "))
        values.append(val)
    scaled_input = scaler.transform([values])
    pred = model.predict(scaled_input)
    print("\nPredicted Class:", pred[0])

"""# **Multiple linear regression**"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
 data = {
     "Hours_studied": [5, 3, 8, 4, 6, 9, 2, 1, 10, 7],
     "Attendence": [90, 60, 100, 70, 85, 95, 40, 30,99,80],
     "Exam_score":[85, 50,92,65,80,95,35,25,98,88]
 }
 df = pd.DataFrame(data)
 X = df[["Hours_studied", "Attendence"]]
 y = df["Exam_score"]
 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
 model = LinearRegression()
 model.fit(X_train, y_train)
 print("Intercept (b0):", model.intercept_)
 print("Coefficients (b1, b2):", model.coef_)
 y_pred = model.predict(X_test)
 print("R2 score:", r2_score(y_test, y_pred))
 print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
results = pd.DataFrame({"Actual": y_test, "Predicted": y_pred})
print(results)

#Multiple linear regression
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
data = {
    "Length": [5,10,30,22,54,72,91,21,74],
    "Breadth": [5,7 ,33,24,58,63,90,21,78],
    "Area": [10, 20, 30, 50, 60, 70, 80, 90, 100]
}
df = pd.DataFrame(data)
X = df[["Length", "Breadth"]]
y = df["Area"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)
print("Intercept (b0):", model.intercept_)
print("Coefficients (b1, b2):", model.coef_)
y_pred = model.predict(X_test)
print("R2 score:", r2_score(y_test, y_pred))
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
results = pd.DataFrame({"Actual": y_test, "Predicted": y_pred})
print(results)

"""# **KNN Accuracy Prediction**"""

from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import numpy as np
digits = datasets.load_digits()       # 1797 image of 8*8 handwritten digits
X = digits.data                       # Flattened 64 features (8*8 pixels)
y = digits.target                     # labels : digits

print("Dataset Shape:", X.shape)
print("Target Shape:", y.shape)

# show first sample image
plt.gray()
plt.matshow(digits.images[0])
plt.title(f"Smaple Digits: { digits.target[0]}")
plt.show()

# split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train KNN model
k = 5
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)

# Predict on test data
y_pred = knn.predict(X_test)

print("\nModel Accuracy:", {accuracy_score(y_test, y_pred)})
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))


# Predict on nem image (from test set)
sample_index = 10 #pick any image from test set
sample_image = X_test[sample_index].reshape(1, -1)
predicted_label = knn.predict(sample_image)
print(f"\nPrediction for test image[{sample_index}] {predicted_label[0]}")
print(f"Actual label {y_test[sample_index]}")
plt.gray()
plt.matshow(digits.images[y_test[sample_index]])
plt.title(f"Predicted: {predicted_label[0]} | Actual: {y_test[sample_index]}")
plt.show()

# KNN on Breast Cancer Dataset
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
df = pd.read_csv("breast_cancer.csv")
print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
print("\nFirst 5 rows:\n", df.head())
target_column = input("\nEnter the name of the target (output) column: ")
X = df.drop(columns=[target_column])
y = df[target_column]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
k = int(input("\nEnter number of neighbors (k): "))
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, zero_division=0))
try:
    choice = input("\nDo you want to test with a new sample? (yes/no): ").strip().lower()
    if choice == 'yes':
        print("Enter feature values one by one (same order as columns):")
        new_data = []
        for col in X.columns:
            val = float(input(f"{col}: "))
            new_data.append(val)
        new_data = scaler.transform([new_data])
        prediction = knn.predict(new_data)
        print(f"\nPredicted Class: {prediction[0]}")
except Exception as e:
    print("Error in prediction:", e)

"""**KNN Algorithm implementation**"""

import numpy as np
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
X = np.array([
    [1, 2],
    [2, 3],
    [3, 3],
    [6, 5],
    [7, 7],
    [8, 6]
])
y = np.array([0, 0, 0, 1, 1, 1])
k = 3  # number of neighbors
model = KNeighborsClassifier(n_neighbors=k)
model.fit(X, y)
new_point = np.array([[5, 5]])
prediction = model.predict(new_point)
print("New point:", new_point)
print("Predicted class:", prediction[0])
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', label='Training points')
plt.scatter(new_point[:, 0], new_point[:, 1], c='green', marker='*', s=200, label='New Point')
plt.title('KNN Classification Example')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

# Decision Tree Algorithm
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt
X = np.array([
    [22, 20000],
    [25, 25000],
    [28, 30000],
    [35, 40000],
    [40, 50000],
    [45, 60000],
    [50, 80000],
    [60, 100000]
])
y = np.array([0, 0, 0, 1, 1, 1, 1, 1])
model = DecisionTreeClassifier(criterion='entropy', random_state=0)
model.fit(X, y)
new_data = np.array([[30, 35000]])
prediction = model.predict(new_data)
print("New Data:", new_data)
print("Predicted Class:", prediction[0])
plt.figure(figsize=(8,6))
tree.plot_tree(model, feature_names=['Age', 'Salary'], class_names=['No', 'Yes'], filled=True)
plt.title("Decision Tree Classifier")
plt.show()

"""# **Naive Bayes Classification**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report
data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast',
                'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool',
                    'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal',
                 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong',
             'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'Play Cricket': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes',
                   'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}
df = pd.DataFrame(data)
print(df.head())
label_encoders = {}
for column in df.columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le
X = df[['Outlook', 'Temperature', 'Humidity', 'Wind']]
y = df['Play Cricket']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)
model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
sample = pd.DataFrame(
    [[label_encoders['Outlook'].transform(['Rain'])[0],
      label_encoders['Temperature'].transform(['Mild'])[0],
      label_encoders['Humidity'].transform(['Normal'])[0],
      label_encoders['Wind'].transform(['Weak'])[0]]],
    columns=['Outlook', 'Temperature', 'Humidity', 'Wind']
)

prediction = model.predict(sample)
predicted_label = label_encoders['Play Cricket'].inverse_transform(prediction)
print("\nPrediction for new sample (Rain, Mild, Normal, Weak):", predicted_label[0])

"""# **# Support Vector Machine (SVM) implementation**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
data = {
    'Weight': [150, 170, 140, 130, 180, 120, 200, 110, 190, 160],
    'Texture': ['Smooth', 'Smooth', 'Rough', 'Rough', 'Smooth', 'Rough', 'Smooth', 'Rough', 'Smooth', 'Rough'],
    'Fruit': ['Apple', 'Apple', 'Orange', 'Orange', 'Apple', 'Orange', 'Apple', 'Orange', 'Apple', 'Orange']
}
df = pd.DataFrame(data)
print(df)
le_texture = LabelEncoder()
le_fruit = LabelEncoder()
df['Texture'] = le_texture.fit_transform(df['Texture'])
df['Fruit'] = le_fruit.fit_transform(df['Fruit'])
X = df[['Weight', 'Texture']]
y = df['Fruit']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = SVC(kernel='linear')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
new_sample = pd.DataFrame([[155, le_texture.transform(['Smooth'])[0]]], columns=['Weight', 'Texture'])
prediction = model.predict(new_sample)
predicted_label = le_fruit.inverse_transform(prediction)
print(f"\nPrediction for new fruit (Weight=155, Texture=Smooth): {predicted_label[0]}")

#Principal Component Analysis (PCA)
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
data = {
    'Math': [85, 90, 78, 92, 88, 76, 95, 89, 84, 91],
    'Science': [80, 85, 75, 89, 83, 70, 92, 88, 79, 86],
    'English': [78, 82, 72, 88, 80, 68, 90, 85, 75, 83]
}
df = pd.DataFrame(data)
print(df)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)
pca = PCA(n_components=2)  # Reduce to 2 dimensions
pca_data = pca.fit_transform(scaled_data)
pca_df = pd.DataFrame(data=pca_data, columns=['PC1', 'PC2'])
print("\nðŸ”¹ Data after PCA (2 components):")
print(pca_df)
print("\nExplained Variance Ratio:", pca.explained_variance_ratio_)
plt.figure(figsize=(6, 5))
plt.scatter(pca_df['PC1'], pca_df['PC2'], color='blue')
plt.title('PCA - Dimensionality Reduction')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True)
plt.show()

"""# **Bagging**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score, classification_report
data = {
    'Age': [18, 22, 25, 28, 30, 35, 40, 45, 50, 55, 60, 23, 27, 32, 38],
    'Screen_Time': [5, 6, 7, 8, 4, 3, 2, 1, 2, 1, 1, 6, 7, 3, 2],
    'Buys_Smartwatch': ['Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'No']
}
df = pd.DataFrame(data)
print(df)
df['Buys_Smartwatch'] = df['Buys_Smartwatch'].map({'No': 0, 'Yes': 1})
X = df[['Age', 'Screen_Time']]
y = df['Buys_Smartwatch']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
base_model = DecisionTreeClassifier(random_state=42)
bagging_model = BaggingClassifier(
    estimator=base_model,
    n_estimators=10,       # number of decision trees
    random_state=42,
    bootstrap=True
)
bagging_model.fit(X_train, y_train)
y_pred = bagging_model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
sample = pd.DataFrame([[26, 7]], columns=['Age', 'Screen_Time'])
prediction = bagging_model.predict(sample)
print(f"Prediction for new customer (Age=26, Screen_Time=7 hrs): {'Buys Smartwatch' if prediction[0]==1 else 'Does Not Buy Smartwatch'}")

"""# **Boosting Classifier**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, classification_report
data = {
    'Daily_Steps': [2000, 4000, 6000, 8000, 10000, 12000, 3000, 5000, 9000, 11000, 7000, 13000, 4000, 1000, 9500],
    'Sleep_Hours': [5, 6, 7, 7, 8, 8, 5, 6, 7, 8, 7, 9, 6, 4, 8],
    'Buys_FitnessBand': ['No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes']
}
df = pd.DataFrame(data)
print(df)
df['Buys_FitnessBand'] = df['Buys_FitnessBand'].map({'No': 0, 'Yes': 1})
X = df[['Daily_Steps', 'Sleep_Hours']]
y = df['Buys_FitnessBand']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
base_model = DecisionTreeClassifier(max_depth=1, random_state=42)
boosting_model = AdaBoostClassifier(
    estimator=base_model,
    n_estimators=20,        # number of weak learners
    learning_rate=1.0,
    random_state=42
)
boosting_model.fit(X_train, y_train)
y_pred = boosting_model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
sample = pd.DataFrame([[8500, 7]], columns=['Daily_Steps', 'Sleep_Hours'])
prediction = boosting_model.predict(sample)
print(f"\nPrediction for new person (8500 steps, 7 hrs sleep): {'Buys Fitness Band' if prediction[0]==1 else 'Does Not Buy'}")

"""# **DBSCAN Clustering**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
X = np.array([
    [1, 2], [2, 2], [2, 3],       # Cluster 1
    [8, 7], [8, 8], [7, 8],       # Cluster 2
    [15, 5], [16, 6], [16, 5],    # Cluster 3
    [25, 25]                      # Noise point (outlier)
])
df = pd.DataFrame(X, columns=['X', 'Y'])
print(df)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
dbscan = DBSCAN(eps=0.8, min_samples=2)  # eps=neighborhood radius, min_samples=min points per cluster
labels = dbscan.fit_predict(X_scaled)
df['Cluster'] = labels
print(df)
plt.figure(figsize=(7,5))
plt.scatter(df['X'], df['Y'], c=df['Cluster'], cmap='viridis', s=100)
plt.title("DBSCAN Clustering Result")
plt.xlabel("X")
plt.ylabel("Y")
for i, label in enumerate(df['Cluster']):
    if label == -1:
        plt.scatter(df['X'][i], df['Y'][i], c='red', marker='x', s=200, label='Noise' if 'Noise' not in plt.gca().get_legend_handles_labels()[1] else None)
plt.legend()
plt.show()